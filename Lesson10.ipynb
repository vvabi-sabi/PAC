{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Что такое нейронная сеть\n",
    "\n",
    "**Нейронная сеть** — это последовательность нейронов, соединенных между собой синапсами. Структура нейронной сети пришла в мир программирования прямиком из биологии. Благодаря такой структуре, машина обретает способность анализировать и даже запоминать различную информацию. Нейронные сети также способны не только анализировать входящую информацию, но и воспроизводить ее из своей памяти.  \n",
    "![Нейрон](http://aboutyourself.ru/assets/sinaps.jpg)\n",
    "**Аксон** — длинный отросток нейрона. Приспособлен для проведения возбуждения и информации от тела нейрона к нейрону или от нейрона к исполнительному органу.  \n",
    "**Дендриты** — короткие и сильно разветвлённые отростки нейрона, служащие главным местом для образования влияющих на нейрон возбуждающих и тормозных синапсов (разные нейроны имеют различное соотношение длины аксона и дендритов), и которые передают возбуждение к телу нейрона. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Модель нейронной сети\n",
    "### Синапс нейронной сети\n",
    "img src=\"images/LessonsII/synaps.png\" alt=\"Synaps\" height=30% width=30%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Перцептрон\n",
    "\n",
    "<img src=\"https://c.mql5.com/2/41/512210577402.png\" alt=\"Искуственный нейрон\" width=40% height=40% >\n",
    "\n",
    "$X$ - входящий вектор признаков  \n",
    "$W$ - веса модели  \n",
    "$b$ - сдвиг модели   \n",
    "$y$ - результат модели  \n",
    "$\\sigma$ - функция активации\n",
    "\n",
    "**<center>Смещение</center>**\n",
    "$$\n",
    "X_0 = 1\n",
    "$$\n",
    "$$\n",
    "S = \\sum_{i=0}^nX_iw_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x = [x1, x2, x3]; W = [w1, w2, w3]\n",
    "$$\n",
    "\\vec x * \\vec W = x_1w_1 + x_2w_2 + x_3w_3\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Полносвязная неронная сеть\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3d/Neural_network.svg/1200px-Neural_network.svg.png\" alt=\"FCNeuralNetwork\" width=30% height=30%>\n",
    "Схема простой нейросети. Зелёным цветом обозначены входные нейроны, голубым — скрытые нейроны, жёлтым — выходной нейрон\n",
    "\n",
    "$$\n",
    "\\vec {h_t} = W_h \\vec x\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec h = F_h(\\vec{h_t})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec {y_t} = W_y \\vec h\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\vec y = F_y(\\vec{y_t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Полносвязная нейронная сеть\n",
    "\n",
    "<img src=\"https://neurohive.io/wp-content/uploads/2018/10/obuchenie-neironnyh-setei-glubokoe.gif\" alt=\"FCNeuralNetwork\" width=60% height=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://adamharley.com/nn_vis/mlp/2d.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Функция активации\n",
    "## Сигмоида\n",
    "<img src=\"images/LessonsII/sigmoid.png\" width=30% height=30% />\n",
    "$$\n",
    "F(x) = {1 \\over 1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Гиперболический тангенс\n",
    "<img src=\"images/LessonsII/tanh.png\" width=30% height=30% />\n",
    "\n",
    "$$\n",
    "F(x) = {e^{2x}-1 \\over e^{2x}+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## ReLU (rectified linear unit)\n",
    "<img src=\"images/LessonsII/ReLU.png\" width=50% height=50% />\n",
    "$$\n",
    "F(x) = max(0, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Softmax\n",
    "\n",
    "Применяется в тех случаях, когда необходимо, чтобы сумма элементов была равно 1, а каждый элемент принадлежал интервалу \\[0; 1\\] (для задач классификации)\n",
    "$$\n",
    "F(x) = {e^{x} \\over \\sum_j e^{x_j}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Метод Обратного Распространения\n",
    "img src=\"https://habrastorage.org/files/dad/168/f54/dad168f54a2d4cf0b6508200eda50eef.png\" alt=\"Example\" height=30% width=30%>\n",
    "Ошибка выходного нейрона\n",
    "$$\n",
    "\\delta_{output} = (out_{pred} - out_{y})f'(in)\n",
    "$$\n",
    "Ошибка внутренного нейрона\n",
    "$$\n",
    "\\delta_{hid} = f'(in)\\sum_i(w_i\\delta_i)\n",
    "$$\n",
    "Градиент ошибки нейрона\n",
    "$$\n",
    "grad_a^b = out_a*\\delta_b\n",
    "$$\n",
    "Изменение весов\n",
    "$$\n",
    "\\Delta w_t = E*grad_a^b+\\alpha*\\Delta w_{t-1}\n",
    "$$\n",
    "\n",
    "*E* - скорость обучения  \n",
    "$\\alpha$ - момент"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/LessonsII/common_act_func.png\" width=70% height=70% />\n",
    " Скорость обучения logistic-ой функции активации низкая с определенного момента (x > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "### Значение момента\n",
    "#![Momentum](https://habrastorage.org/files/4d9/684/061/4d96840618dc44768e57e892033a119a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Функция потерь\n",
    "### Регрессия\n",
    "**Средняя абсолютная ошибка**\n",
    "$$\n",
    "MAE = {1\\over n}\\sum_{i=1}^n|y_i - pred_i|\n",
    "$$\n",
    "*n - число признаков в выходном векторе*  \n",
    "*pred - предсказанный вектор*\n",
    "  \n",
    "**Средняя квадратичная ошибка**\n",
    "$$\n",
    "MSE = {1\\over n}\\sum_{i=1}^n(y_i - pred_i)^2\n",
    "$$\n",
    "**Средняя абсолютная процентная ошибка**\n",
    "$$\n",
    "MAPE = {100\\% \\over n}\\sum_{i=1}^n{|y_i - pred_i|\\over y_i}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000000000004"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MAE\n",
    "def mean_absolute_error(y, pred): #\n",
    "    diff = y - pred # находим разницу между  наблюдаемыми значениями (y) и прогнозируемыми (pred)\n",
    "    abs_diff = np.absolute(diff) # находим абсолютную разность между прогнозами и фактическими наблюдениями.\n",
    "    mean_diff = abs_diff.mean() # находим среднее значение\n",
    "    return mean_diff\n",
    "\n",
    "y = np.array([1.1,2,1.7]) # создаем список актуальных значений\n",
    "pred = np.array([1,1.7,1.5]) # список прогнозируемых значений\n",
    "\n",
    "# mean_absolute_error(y, pred) # sklearn\n",
    "mean_absolute_error(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04666666666666667"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MSE\n",
    "def mean_squared_error(y, pred): # функция \n",
    "   diff = y - pred # находим разницу между  наблюдаемыми значениями (y) и прогнозируемыми (pred)\n",
    "   differences_squared = diff ** 2 # возводим в квадрат (чтобы избавиться от отрицательных значений)\n",
    "   mean_diff = differences_squared.mean() # находим среднее значение\n",
    "   \n",
    "   return mean_diff\n",
    "\n",
    "y = np.array([1.1,2,1.7]) \n",
    "pred = np.array([1,1.7,1.5]) \n",
    "\n",
    "# mean_squared_error(y, pred) # sklearn\n",
    "mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21602468994692867"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RMSE\n",
    "def root_mean_squared_error(y, pred):\n",
    "   diff = y - pred # находим разницу между  наблюдаемыми значениями (y) и прогнозируемыми (pred)\n",
    "   differences_squared = diff ** 2 # возводим в квадрат\n",
    "   mean_diff = differences_squared.mean() # находим среднее значение\n",
    "   rmse_val = np.sqrt(mean_diff) # извлекаем квадратный корень\n",
    "   return rmse_val\n",
    "\n",
    "y = np.array([1.1,2,1.7])\n",
    "pred = np.array([1,1.7,1.5])\n",
    "\n",
    "# mean_squared_error(y, pred, squared = False) #Если установлено значение False, функция возвращает значение RMSE.\n",
    "root_mean_squared_error(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Классификация\n",
    "**Бинарная кросс энтропия**\n",
    "$$\n",
    "CE = -y_1log(pred_1)-(1-y_1)log(1-pred_1)\n",
    "$$\n",
    "**Категориальная кросс энтропия**\n",
    "$$\n",
    "CE = -\\sum_j^Cy_jlog(pred_j)\n",
    "$$\n",
    "*C - число классов*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21616187468057912"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def binary_cross_entropy(y, pred):\n",
    "    y = np.array([1 if el == \"Cat\" else 0 for el in y]) # [[0 1 1 0]]\n",
    "    pred = np.array([el[0] for el in pred])\n",
    "    CE = -y*np.log(pred) - (1 - y)*np.log((1-pred))\n",
    "    return CE.mean()\n",
    "\n",
    "labels = {\"Cat\": [1, 0], \"Dog\":[0,1]}\n",
    "\n",
    "def encode_label(y):\n",
    "    return np.array([labels[key] for key in y])\n",
    "\n",
    "def cross_entropy(y, pred):\n",
    "    y = encode_label(y) # [[0, 1], [1, 0], [1, 0], [0, 1]]\n",
    "    pred = np.array(pred)\n",
    "    CE = -np.sum(y*np.log(pred), axis=-1)\n",
    "    return CE.mean()\n",
    "\n",
    "y = [\"Dog\", \"Cat\", \"Cat\", \"Dog\"] # список правильных меток классов\n",
    "pred = [[.1, .9], [.9, .1], [.8, .2], [.35, .65]] # [P(dog), P(cat)] # список вероятностей, предсказанных моделью.\n",
    "\n",
    "# (Первый аргумент в вызове функции — это список правильных меток классов для каждого входа. Второй аргумент — это список вероятностей, предсказанных моделью.)\n",
    "# log_loss(y, pred) # sklearn \n",
    "cross_entropy(y, pred) # binary_cross_entropy(y, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "img src=\"images/LessonsII/cross_entr.png\" alt=\"SoftMax\" height=80% width=80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Сверточные нейронные сети\n",
    "\n",
    "#![Архитектура](https://upload.wikimedia.org/wikipedia/commons/5/55/%D0%90%D1%80%D1%85%D0%B8%D1%82%D0%B5%D0%BA%D1%82%D1%83%D1%80%D0%B0_%D1%81%D0%B2%D0%B5%D1%80%D1%82%D0%BE%D1%87%D0%BD%D0%BE%D0%B9_%D0%BD%D0%B5%D0%B9%D1%80%D0%BE%D0%BD%D0%BD%D0%BE%D0%B9_%D1%81%D0%B5%D1%82%D0%B8.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Слой свёртки (convolutional layer) — это основной блок свёрточной нейронной сети. Слой свёртки включает в себя для каждого канала свой фильтр, ядро свёртки которого обрабатывает предыдущий слой по фрагментам (суммируя результаты поэлементного произведения для каждого фрагмента). Весовые коэффициенты ядра свёртки (небольшой матрицы) неизвестны и устанавливаются в процессе обучения.\n",
    "#![Свёртка](https://neurohive.io/wp-content/uploads/2018/07/2d-covolutions.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Свёртка\n",
    "img src=\"images/LessonsII/convolution.jpeg\" alt=\"Conv\" height=40% width=40%>\n",
    "$$O_{i, j} = f(K*I_{i-s:i+s, j-s:j+s})$$\n",
    "\n",
    "$K$ - ядро свёртки  \n",
    "$I$ - входной тезнор  \n",
    "$I_{i-s:i+s, j-s:j+s}$ - срез тензора размера ядра свёртки с центром в точке (i, j)  \n",
    "$O_{i, j}$ - результат свёртки (значение выходного тензора в точке (i, j))  \n",
    "$f$ - функция активации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Граф вычислений\n",
    "img src=\"images/LessonsII/graph.png\" alt=\"gr\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обратное распространение ошибки. Пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$f(x,w) = 1 + e^{w_1x+w_0}$$\n",
    "img src=\"images/LessonsII/back1_1.png\" alt=\"Back1\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Начальное состояение весов и входного значения\n",
    "img src=\"images/LessonsII/back1_2.png\" alt=\"back1_2\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Посчитаем значения по прямому проходу\n",
    "img src=\"images/LessonsII/back1_3.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Для удобства обозначим узлы (подфункции)\n",
    "img src=\"images/LessonsII/back1_4.png\" alt=\"back1_4\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Обратное распространение. Пусть df = 1. Тогда \n",
    "$$dc = {df \\over dc}df = (c+1)'_{c} * 1 = 1$$\n",
    "$$db = {dc \\over db}dc = (e^b)'_{b} * 1 = e^3 = 20$$\n",
    "img src=\"images/LessonsII/back1_5.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$dw_0 = {db \\over dw_0}db = (a+w_0)'_{w_0} * 20 = 20$$\n",
    "$$da = {db \\over da}db = (a+w_0)'_{a} * 20 = 20$$\n",
    "img src=\"images/LessonsII/back1_6.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$dw_1 = {da \\over dw_1}da = (x*w_1)'_{w_1} * 20 = x*20 = 20$$\n",
    "$$dx = {da \\over dx}da = (x*w_1)'_{x} * 20 = w_1*20 = 40$$\n",
    "img src=\"images/LessonsII/back1_7.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обратное распространение ошибки. Пример с матрицами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$f={1 \\over 2} ||X*W||^2_2$$\n",
    "img src=\"images/LessonsII/back2_1.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Начальное состояение весов и входного значения\n",
    "img src=\"images/LessonsII/back2_2.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Посчитаем значения по прямому проходу\n",
    "img src=\"images/LessonsII/back2_3.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Для удобства обозначим узлы (подфункции)  \n",
    "img src=\"images/LessonsII/back2_4.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Обратное распространение. Пусть df = 1. Тогда \n",
    "$$db = {df \\over db}df = ({1 \\over 2}b)'_{b} * 1 = 0.5$$\n",
    "$$da_{ij} = {db \\over da_{ij}}db = (a_{ij}^2)'_{a_{ij}} * 0.5 = a_{ij} $$\n",
    "или $$\\nabla_a f = a$$\n",
    "\n",
    "img src=\"images/LessonsII/back2_5.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\nabla_w f = X^T \\nabla_a f$$\n",
    "\n",
    "img src=\"images/LessonsII/back2_6.png\" height=60% width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Архитектуры нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Датасет ImageNet\n",
    "ImageNet — набор данных, состоящий из более чем 15 миллионов размеченных высококачественных изображений, разделенных на 22000 категорий. Изображения были взяты из интернета и размечены вручную людьми-разметчиками \n",
    "\n",
    "img src=\"https://alexisbcook.github.io/assets/cifar10.png\" alt=\"VGG\" height=60% width=60% />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## VGG\n",
    "\n",
    "VGG16 — одна из самых знаменитых моделей, отправленных на соревнование ILSVRC-2014. Она является улучшенной версией AlexNet, в которой заменены большие фильтры (размера 11 и 5 в первом и втором сверточном слое, соответственно) на несколько фильтров размера 3х3, следующих один за другим. Сеть VGG16 обучалась на протяжении нескольких недель при использовании видеокарт NVIDIA TITAN BLACK.\n",
    "\n",
    "img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network-1-e1542973058418.jpg\" alt=\"VGG\" height=60% width=60% />\n",
    "img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-2.png\" alt=\"VGG Plain\" height=50% width=50% />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ResNet\n",
    "Основой сети ResNet является Residual-блок (остаточный блок) с shortcut-соединением, через которое данные проходят без изменений. Res-блок представляет собой несколько сверточных слоев с активациями, которые преобразуют входной сигнал x в F(x). Shortcut-соединение — это тождественное преобразование x -> x.\n",
    "\n",
    "img src=\"https://www.researchgate.net/publication/322621180/figure/fig2/AS:584852684410885@1516451154473/The-representation-of-model-architecture-image-for-ResNet-152-VGG-19-and-two-layered.png\" alt=\"ResBlock\" height=30% width=30% />\n",
    "img src=\"https://neurohive.io/wp-content/uploads/2019/01/resnet-neural-e1548772388921.png\" alt=\"ResBlock\" height=30% width=30% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MobileNet V2\n",
    "\n",
    "Особенностью данной архитектуры является отсутствие max pooling-слоёв. Вместо них для снижения пространственной размерности используется свёртка с параметром stride, равным 2. Также изменение свёртки уменьшает количество операций сети\n",
    "\n",
    "img src=\"https://miro.medium.com/max/1016/1*5iA55983nBMlQn9f6ICxKg.png\" alt=\"MobNet\" height=30% width=30% />\n",
    "\n",
    "\n",
    "img src=\"https://habrastorage.org/webt/wl/yo/sz/wlyoszqnws58itd4ojt1cqt7sng.png\" alt=\"MobBlock\" height=30% width=30% />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# U-Net\n",
    "U-Net — это свёрточная нейронная сеть, которая была создана в 2015 году для сегментации биомедицинских изображений в отделении Computer Science Фрайбургского университета. Архитектура сети представляет собой полносвязную свёрточную сеть, модифицированную так, чтобы она могла работать с меньшим количеством примеров (обучающих образов) и делала более точную сегментацию.\n",
    "img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" alt=\"UNet\" height=50% width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Рекуррентные нейронные сети (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Рекуррентные нейронные сети (RNN) — вид нейронных сетей, где связи между элементами образуют направленную последовательность. Благодаря этому появляется возможность обрабатывать серии событий во времени или последовательные пространственные цепочки. В отличие от многослойных перцептронов, рекуррентные сети могут использовать свою внутреннюю память для обработки последовательностей произвольной длины. Поэтому сети RNN применимы в таких задачах, где нечто целостное разбито на части, например: распознавание рукописного текста или распознавание речи. \n",
    "img src=\"https://habrastorage.org/web/5c8/0fa/c22/5c80fac224d449209d888d18ea1111a8.png\" alt=\"RNN\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Внутреннее устройство простых RNN\n",
    "Входной вектор и вектор внутренней памяти объединяются и отправляются на полносвязный слой с функцией активации *tanh*\n",
    "img src=\"https://habrastorage.org/web/47d/ee6/2c3/47dee62c3af8498c946befa1f3330d90.png\" alt=\"RNN In\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# LSTM\n",
    "Долгая краткосрочная память (Long short-term memory; LSTM) – особая разновидность архитектуры рекуррентных нейронных сетей, способная к обучению долговременным зависимостям.\n",
    "img src=\"https://habrastorage.org/web/67b/04f/73b/67b04f73b4c34ba38edfa207e09de07c.png\" alt=\"LSTM\">\n",
    "* «вентиль забывания» контролирует меру сохранения значения в памяти\n",
    "* «входной вентиль» контролирует меру вхождения нового значения в память\n",
    "* «выходной вентиль» контролирует меру того, в какой степени значение, находящееся в памяти, используется при расчёте выходной функции активации для блока"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# GRU\n",
    "Управляемые рекуррентные блоки (Gated Recurrent Units, GRU) — механизм вентилей для рекуррентных нейронных сетей, представленный в 2014 году. Было установлено, что его эффективность при решении задач моделирования музыкальных и речевых сигналов сопоставима с использованием долгой краткосрочной памяти (LSTM). По сравнению с LSTM у данного механизма меньше параметров, т.к. отсутствует выходной вентиль.\n",
    "<img src=\"https://www.data-blogger.com/wp-content/uploads/2017/08/gru.png\" alt=\"GRU\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Переобучение\n",
    "**Недообучение** — нежелательное явление, возникающее при решении задач обучения по прецедентам, когда алгоритм обучения не обеспечивает достаточно малой величины средней ошибки на обучающей выборке. Недообучение возникает при использовании недостаточно сложных моделей.\n",
    "\n",
    "**Переобучение** (overtraining, overfitting) — нежелательное явление, возникающее при решении задач обучения по прецедентам, когда вероятность ошибки обученного алгоритма на объектах тестовой выборки оказывается существенно выше, чем средняя ошибка на обучающей выборке. Переобучение возникает при использовании избыточно сложных моделей.\n",
    "img src=\"images/LessonsII/Overfitting.svg.png\" width=80% height=80% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Процесс обучения\n",
    "img src=\"images/LessonsII/Overfitting.curve.png\" width=30% height=30% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Возможные решения при переобучении:\n",
    "* Увеличение количества данных в наборе;\n",
    "* Уменьшение количества параметров модели (количество параметров модели (весов) была в 2 - 3 раза меньше числа примеров обучающего множества);\n",
    "* Добавление регуляризации / увеличение коэффициента регуляризации.\n",
    "\n",
    "### Возможные решения при недообучении:\n",
    "* Добавление новых параметров модели;\n",
    "* Использование для описания модели функций с более высокой степенью;\n",
    "* Уменьшение коэффициента регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Аугментация данных\n",
    "\n",
    "**Аугментация данных** (data augmentation) – это методика создания дополнительных обучающих данных из имеющихся данных. Для достижения хороших результатов глубокие сети должны обучаться на очень большом объеме данных. Следовательно, если исходный обучающий набор содержит ограниченное количество изображений, необходимо выполнить аугментацию, чтобы улучшить результаты модели.\n",
    "\n",
    "Можно использовать следующие искажения:\n",
    "* Геометрические (афинные, проективные, ...);\n",
    "* Яркостные/цветовые;\n",
    "* Замена фона;\n",
    "* Искажения, характерные для решаемой задачи: блики, шумы, размытие и т. д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Обучение с подкреплением\n",
    "Обучение с подкреплением — это метод машинного обучения, при котором происходит обучение модели, которая не имеет сведений о системе, но имеет возможность производить какие-либо действия в ней. Действия переводят систему в новое состояние и модель получает от системы некоторое вознаграждение\n",
    "\n",
    "img src=\"https://www.kdnuggets.com/images/reinforcement-learning-fig1-700.jpg\" alt=\"RF\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Обучение модели\n",
    "img src=\"https://media.giphy.com/media/PH67wPdphHPk4/giphy.gif\" alt=\"Walking\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ресурсы для углублённого изучения темы\n",
    "* [Видеолекции Семёна Козлова](https://www.youtube.com/channel/UCQj_dwbIydi588xrfjWSL5g/featured)\n",
    "* [Курс по машинному обучению](https://www.coursera.org/learn/machine-learning)\n",
    "* [Цикл статей об истории нейронных сетей](http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Задания\n",
    "\n",
    "См. Lesson 10.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Лабораторная работа 10. Матчасть DL\n",
    "\n",
    "Задача: реализовать и обучить нейронную сеть, состоящую из 2 нейронов, предсказывать значения функции XOR. При выполнении лабораторной запрещается использовать фреймворки для глубокого обучения (как PyTorch, Tensorflow, Caffe, Theano и им подобные).  \n",
    "  \n",
    "В первую очередь ознакомиться с [этим](https://towardsdatascience.com/implementing-the-xor-gate-using-backpropagation-in-neural-networks-c1f255b4f20d) материалом.\n",
    "  \n",
    "Что необходимо реализовать, используя знания и фрагменты кода из ссылки выше:  \n",
    "1. Класс Neuron, имеющий вектор весов self._weigths\n",
    "2. Два метода класса Neuron: forward(x), backward(x, loss) - реализующих прямой и обратный проход по нейронной сети. \n",
    "   Метод forward должен реализовывать логику работу нейрона: умножение входа на вес self._weigths, сложение и функцию активации сигмоиду. \n",
    "   Метод backward должен реализовывать взятие производной от сигмоиды и используя состояние нейрона обновить его веса.\n",
    "3. Реализовать с помощью класса Neuron нейронную сеть с архитектурой из трёх нейронов, предложенную в статье:\n",
    "<img src=\"images/LessonsII/Neuron.png\" alt=\"Neuron\" height=60% width=60%>\n",
    "\n",
    "   Для красоты обернуть в класс Model с методами forward и backward, реализующими правильное взаимодействие нейронов на прямом и обратном проходах.\n",
    "5. Реализовать тренировочный цикл следующего вида:\n",
    "\n",
    "```  \n",
    "цикл (обучающие данные):\n",
    "\ty = model.forward(x)\n",
    "\terr = loss(y, label)\n",
    "\tmodel.backward(x, err)\n",
    "```\n",
    "В итоге обучения должны предсказываться значения аналогичные описанным в статье.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Слайд-шоу",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
